# 181222-day2-quiz

## 1. 為什麼 Python 程式語言要分成 Python2, Python3 不同的版本，使用上有什麼問題？
python有兩個主要的版本，python2和python3，但是是python又不同於其他語言，向下兼容，python3是不向下兼容的，但是絕大多數組件和擴展都是基於python2
## 2. 完成以下填空：

- 使用者在看瀏覽網站的操作行為，使基於【＿HTTP＿】  協定：
- 使用者在瀏覽器上輸入網址…
- [使用者/Client side] 發送 【＿Request＿】 給 [網站伺服器/ Server side]
- [網站伺服器/ Server side] 回傳 【＿Response＿】給 [使用者/Client side]
瀏覽器將內容處理之後顯示給使用者…

## 3. 請問動態網頁爬蟲跟靜態網頁爬蟲主要的差別是什麼？
靜態網頁
超文件標示語言HyperText Markup Language，簡稱：HTML，HTML是網站建置的基礎技術，常與CSS與Javascript配合成一個適合觀看的網頁，讓瀏覽器去讀取，一般判斷方式為網頁副檔名為html或htm皆為靜態網頁，靜態網頁的優勢為容易為搜尋引擎所接受，所以很多動態網頁會將動態網頁轉變成靜態方式，就是所謂的【偽靜態網頁】來提高搜尋引擎的友善度達到排名優化的成效。

動態網頁
動態網頁主要是搭配伺服器與資料庫共同運作，主要是使用大量編譯的地方，如會員功能、購物車、討論區等等..，意思是指可以與網頁做互動編譯的網頁，動態網頁的內容隨著用戶的輸入和互動而有所不同有Perl、PHP、ASP、JSP、ColdFusion等編譯方式，從而對動態網頁的內容進行改變。

靜態網頁與動態的區別
靜態網頁與動態網頁是可以同時存在一個網站上的，二種語言各有其優勢，靜態網站主要是用於較於簡單，更新的不頻繁的網頁，反之動態網站較適合用於，資料內容較大，更新快速的網頁，讓維護人員可以更方便管理網站，也大幅降低維護成本。

## 4. 動態網頁爬蟲跟靜態網頁爬蟲的爬蟲策略有什麼差別？


靜態網站的資料取得：Requests 或是 urllib 這兩個套件都是用於處理 HTTP 協定的工具。urllib 是內建於 Python 有比較完整的 HTTP 的功能（包含網址編碼、檔案下載之類的），Requests 則比較著重在友善地處理 Request/Response 的傳輸。

網頁資料的解析爬取：BeautifulSoup 與 Pyquery 是用在接到 HTML 字串的 Response 之後，要如何將他們解析為一個 DOM base 的物件使用。lxml 跟 html5lib 是兩個作為讀懂 HTML 字串的解析器（parser）。這兩個套件都支援使用類似 css selector 的方式找資料。XPath 是基於 XML 格式的定位技術，也可以將 HTML 視為是 XML 的方式處理，再使用 XPath 找到需要的資料。

動態網站的資料取得：Selenium 原本是用於網頁測試的瀏覽器模擬工具。但隨著動態網頁/AJAX 的技術方法，僅透過 Requests 的話會遇到 JavaScript loading 的問題，因此可以搭配 Selenium 這樣的瀏覽器模擬工具，來達到執行 JavaScript 的效果。原本的 Selenium 模擬工具需要調用實體的瀏覽器，像是 Chrome、Firefox 之類的，會造成資源與效能的問題。PhantomJS 則是一個在 Selenium 中的虛擬瀏覽器方案，可以在不須打開實體瀏覽器的情況下進行模擬。


## 5. 找一個網站，說明哪些內容是可以透過靜態方式取得？哪些是需要透過動態方式取得？



## 6. 實作題：利用 Yahoo! 電影，取出本週熱門新片的「中英文名稱」、「上映日期」、「期待度」、「滿意度」，還有「封面照片網址」，並印出。用一個你覺得適合的型態存到一個變數內。

from bs4 import BeautifulSoup

movies = []

for p in range(1, 10):

    url = f'https://movies.yahoo.com.tw/movie_intheaters.html?page={p}'
    r = requests.get(url)
    soup = BeautifulSoup(r.text, "html5lib")
    for i, j, k in zip(soup.find_all(class_='release_movie_name'), 
                       soup.find_all(class_='release_movie_time'),
                       soup.find_all(class_="release_foto")):
        d = {}
        d['中文名稱'] = i.find(class_='gabtn').text.replace(' ', '').replace('\n', '')
        d['英文名稱'] = i.find(class_='en').find(class_='gabtn').text.replace(' ', '').replace('\n', '')
        d['期待度'] = i.find(class_='levelbox').span.text
        #d['期待度2'] = i.find_all(class_="leveltext")[0].text.strip().split()[0]
        d['滿意度'] = i.find(class_='leveltext starwithnum').span['data-num'] #data-num=資訊 這時候要用[]
        d['上映日期'] = j.text.replace(' ', '').replace('上映日期：', '')
        d['封面照片網址'] =k.find("img")['src'] #src=資訊 這時候要用[]
        movies.append(d)
print(movies)   

## 7. 實作題：利用 104 人力銀行，計算出台中市用「資料」關鍵字可以找到的公司數量有幾個？

import requests
from bs4 import BeautifulSoup

url = 'https://www.104.com.tw/jobs/search/?ro=0&keyword=%E8%B3%87%E6%96%99&area=6001008000&order=1&asc=0&scstrict=0&scneg=1&page=1&mode=s&jobsource=joblist_b_relevance'

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}

r = requests.get(url, headers = headers)
r.encoding = 'utf-8'

soup = BeautifulSoup(r.text, 'html5lib')


l = []
for i in soup.find_all(class_="b-list-inline b-clearfix"):
    d = {}
    d['company'] = i.find('a').text.replace(' ', '').replace('\n', '')
    l.append(d)
    
print(l)

print('公司數:',len(l))
